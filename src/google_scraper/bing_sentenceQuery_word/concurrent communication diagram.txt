In refers ability different part unit program algorithm problem executed partial order without affecting final outcome
This allows parallel execution concurrent unit significantly improve overall speed execution system
In technical term concurrency refers decomposability property program algorithm problem component unit
A number mathematical model developed general concurrent computation including model
As Leslie Lamport note While execution considered year computer science concurrency began seminal paper introduced problem
The ensuing decade seen huge growth interest
Looking back origin field stand fundamental role played Edsger Dijkstra
Because computation concurrent system interact executed number possible execution path system extremely large resulting outcome
Concurrent use shared source indeterminacy leading issue
Design concurrent system often entail finding reliable technique coordinating execution data exchange execution scheduling minimize maximise
Concurrency theory active field research
One first proposal seminal work early
In year since wide variety formalism developed modeling reasoning concurrency
A number formalism modeling understanding concurrent system developed including Some model concurrency primarily intended support reasoning specification others used entire development cycle including design implementation proof testing simulation concurrent system
Some based others different mechanism concurrency
The proliferation different model concurrency motivated researcher develop way unify different theoretical model
For example Lee demonstrated model used provide common framework defining variety different model concurrency Nielsen Sassone Winskel demonstrated used provide similar unified understanding different model
The Concurrency Representation Theorem Actor model provides fairly general way represent concurrent system closed sense receive communication outside
Other concurrency system modeled Actor model using
The mathematical denotation denoted closed system constructed increasingly better approximation initial behavior called using behavior approximating function construct denotation meaning follows In way mathematically characterized term possible behavior
Various type used help reason concurrent system
Some logic allow assertion made sequence state concurrent system pas
Others build assertion sequence change state
The principal application logic writing specification concurrent system
encompasses programming language algorithm used implement concurrent system
Concurrent programming usually considered general involve arbitrary dynamic pattern communication interaction whereas parallel system generally predefined communication pattern
The base goal concurrent programming include
Concurrent system generally designed operate indefinitely including automatic recovery failure terminate unexpectedly see
Some concurrent system implement form transparent concurrency concurrent computational entity may compete share single resource complexity competition sharing shielded programmer
Because use shared resource concurrent system general require inclusion kind somewhere implementation often underlying hardware control access resource
The use arbiter introduces possibility major implication practice including correctness performance
For example arbitration introduces raise issue cause explosion state space even cause model infinite number state
Some concurrent programming model include
In model thread control explicitly timeslices either system another process

type many calculation execution carried simultaneously
Large problem often divided smaller one solved time
There several different form parallel computing
Parallelism employed many year mainly interest grown lately due physical constraint preventing
As power consumption consequently heat generation computer become concern recent year parallel computing become dominant paradigm mainly form
Parallel computing closely related frequently used together often conflated though two distinct possible parallelism without concurrency concurrency without parallelism multitasking CPU
In parallel computing computational task typically broken several often many similar subtasks processed independently whose result combined afterwards upon completion
In contrast concurrent computing various process often address related task typical separate task may varied nature often require execution
Parallel computer roughly classified according level hardware support parallelism computer multiple within single machine use multiple computer work task
Specialized parallel computer architecture sometimes used alongside traditional processor accelerating specific task
In case parallelism transparent programmer parallelism explicitly particularly use concurrency difficult write sequential one concurrency introduces several new class potential common
different subtasks typically greatest obstacle getting good parallel program performance
A theoretical single program result parallelization given
Traditionally written
To solve problem constructed implemented serial stream instruction
These instruction executed one computer
Only one instruction may execute instruction finished next one executed
Parallel computing hand us multiple processing element simultaneously solve problem
This accomplished breaking problem independent part processing element execute part algorithm simultaneously others
The processing element diverse include resource single computer multiple processor several networked computer specialized hardware combination
dominant reason improvement
The runtime program equal number instruction multiplied average time per instruction
Maintaining everything else constant increasing clock frequency decrease average time take execute instruction
An increase frequency thus decrease runtime program
However power consumption chip given equation switched per clock cycle proportional number transistor whose input change processor frequency cycle per second
Increases frequency increase amount power used processor
Increasing processor power consumption led ultimately May cancellation processor generally cited end frequency scaling dominant computer architecture paradigm
empirical observation number transistor microprocessor double every month
Despite power consumption issue repeated prediction end Moore law still effect
With end frequency scaling additional transistor longer used frequency scaling used add extra hardware parallel computing
Optimally parallelization would number processing element halve runtime doubling second time halve runtime
However parallel algorithm achieve optimal speedup
Most speedup small number processing element flattens constant value large number processing element
The potential speedup algorithm parallel computing platform given Since show small part program parallelized limit overall speedup available parallelization
A program solving large mathematical engineering problem typically consist several parallelizable part several serial part
If part program account runtime get time speedup regardless many processor added
This put upper limit usefulness adding parallel execution unit
When task partitioned sequential constraint application effort effect schedule
The bearing child take nine month matter many woman assigned
Amdahl law applies case problem size fixed
In practice computing resource become available tend get used larger problem larger datasets time spent parallelizable part often grows much faster inherently serial work
In case give le pessimistic realistic assessment parallel performance Both Amdahl law Gustafson law assume running time serial part program independent number processor
Amdahl law assumes entire problem fixed size total amount work done parallel also whereas Gustafson law assumes total amount work done parallel
Understanding fundamental implementing
No program run quickly longest chain dependent calculation known since calculation depend upon prior calculation chain must executed order
However algorithm consist long chain dependent calculation usually opportunity execute independent calculation parallel
Let two program segment
Bernstein condition describe two independent executed parallel
For let input variable output variable likewise
independent satisfy Violation first condition introduces flow dependency corresponding first segment producing result used second segment
The second condition represents second segment produce variable needed first segment
The third final condition represents output dependency two segment write location result come logically last executed segment
Consider following function demonstrate several kind dependency In example instruction executed even parallel instruction instruction us result instruction
It violates condition thus introduces flow dependency
In example dependency instruction run parallel
Bernstein condition allow memory shared different process
For mean enforcing ordering access necessary
Subtasks parallel program often called
Some parallel computer architecture use smaller lightweight version thread known others use bigger version known
However thread generally accepted generic term subtasks
Threads often need update shared
The instruction two program may interleaved order
For example consider following program If instruction executed instruction executed program produce incorrect data
This known
The programmer must use provide
A lock programming language construct allows one thread take control variable prevent thread reading writing variable unlocked
The thread holding lock free execute section program requires exclusive access variable unlock data finished
Therefore guarantee correct program execution program rewritten use lock One thread successfully lock variable V thread proceed V unlocked
This guarantee correct execution program
Locks necessary ensure correct program execution greatly slow program
Locking multiple variable using lock introduces possibility program
An lock multiple variable
If lock lock
If two thread need lock two variable using lock possible one thread lock one second thread lock second variable
In case neither thread complete deadlock result
Many parallel program require subtasks
This requires use
Barriers typically implemented using software lock
One class algorithm known altogether avoids use lock barrier
However approach generally difficult implement requires correctly designed data structure
Not parallelization result
Generally task split thread thread spend portion time communicating
Eventually overhead communication dominates time spent solving problem parallelization splitting workload even thread increase rather decrease amount time required finish
This known
Applications often classified according often subtasks need synchronize communicate
An application exhibit parallelism subtasks must communicate many time per second exhibit parallelism communicate many time per second exhibit rarely never communicate
Embarrassingly parallel application considered easiest parallelize
Parallel programming language parallel computer must also known memory model
The consistency model defines rule operation occur result produced
One first consistency model model
Sequential consistency property parallel program parallel execution produce result sequential program
Specifically program sequentially consistent result execution operation processor executed sequential order operation individual processor appear sequence order specified program
common type consistency model
Software transactional memory borrows concept applies memory access
Mathematically model represented several way
introduced Carl Adam Petri doctoral thesis early attempt codify rule consistency model
Dataflow theory later built upon created physically implement idea dataflow theory
Beginning late developed permit algebraic reasoning system composed interacting component
More recent addition process calculus family added capability reasoning dynamic topology
Logics Lamport mathematical model also developed describe behavior concurrent system
created one earliest classification system parallel sequential computer program known
Flynn classified program computer whether operating using single set multiple set instruction whether instruction using single set multiple set data
The SISD classification equivalent entirely sequential program
The SIMD classification analogous operation repeatedly large data set
This commonly done application
MISD rarely used classification
While computer architecture deal devised application fit class materialized
MIMD program far common type parallel program
According Some machine hybrid category course classic model survived simple easy understand give good first approximation
It widely used scheme
From advent VLSI fabrication technology computer architecture driven doubling amount information processor manipulate per cycle
Increasing word size reduces number instruction processor must execute perform operation variable whose size greater length word
For example processor must add two processor must first add bit integer using standard addition instruction add bit using instruction lower order addition thus processor requires two instruction complete single operation processor would able complete operation single instruction
Historically microprocessor replaced microprocessor
This trend generally came end introduction processor standard computing two decade
Not early advent architecture processor become commonplace
A computer program essence stream instruction executed processor
Without parallelism processor issue le one
These processor known processor
These instruction combined group executed parallel without changing result program
This known parallelism
Advances parallelism dominated computer architecture
All modern processor
Each stage pipeline corresponds different action processor performs instruction stage processor pipeline different instruction different stage completion thus issue one instruction per clock cycle
These processor known processor
The canonical example pipelined processor processor five stage instruction fetch IF instruction decode ID execute EX memory access MEM register write back WB
The processor pipeline
Most modern processor also multiple
They usually combine feature pipelining thus issue one instruction per clock cycle
These processor known processor
Instructions grouped together
similar scoreboarding make use two common technique implementing execution parallelism
Task parallelism characteristic parallel program entirely different calculation performed either different set data
This contrast data parallelism calculation performed different set data
Task parallelism involves decomposition task allocating processor execution
The processor would execute simultaneously often cooperatively
Task parallelism usually scale size problem
Main memory parallel computer either shared processing element single processing element local address space
Distributed memory refers fact memory logically distributed often implies physically distributed well
combine two approach processing element local memory access memory processor
Accesses local memory typically faster access memory
Computer architecture element main memory accessed equal known UMA system
Typically achieved system memory physically distributed
A system property known NUMA architecture
Distributed memory system memory access
Computer system make use fast memory located close processor store temporary copy memory value nearby physical logical sense
Parallel computer system difficulty cache may store value one location possibility incorrect program execution
These computer require system keep track cached value strategically purge thus ensuring correct program execution
one common method keeping track value accessed thus purged
Designing large cache coherence system difficult problem computer architecture
As result shared memory computer architecture scale well distributed memory system
communication implemented hardware several way including via shared either multiported memory shared interconnect network myriad including fat hypercube hypercube one processor node
Parallel computer based interconnected network need kind enable passing message node directly connected
The medium used communication processor likely hierarchical large multiprocessor machine
Parallel computer roughly classified according level hardware support parallelism
This classification broadly analogous distance basic computing node
These mutually exclusive example cluster symmetric multiprocessor relatively common
A processor processor includes multiple called core chip
This processor differs processor includes multiple issue multiple instruction per clock cycle one instruction stream thread contrast processor issue multiple instruction per clock cycle multiple instruction stream
designed use prominent processor
Each core processor potentially superscalar every clock cycle core issue multiple instruction one thread
Intel best known early form
A processor capable simultaneous multithreading includes multiple execution unit processing superscalar issue multiple instruction per clock cycle thread
hand includes single execution unit processing unit issue one instruction time thread
A symmetric multiprocessor SMP computer system multiple identical processor share memory connect via bus
prevents bus architecture scaling
As result SMPs generally comprise processor
Because small size processor significant reduction requirement bus bandwidth achieved large cache symmetric multiprocessor extremely provided sufficient amount memory bandwidth exists
A distributed computer also known distributed memory multiprocessor distributed memory computer system processing element connected network
Distributed computer highly scalable
The term distributed computing lot overlap clear distinction exists
The system may characterized parallel distributed processor typical distributed system run concurrently parallel
A cluster group loosely coupled computer work together closely respect regarded single computer
Clusters composed multiple standalone machine connected network
While machine cluster symmetric difficult
The common type cluster cluster implemented multiple identical computer connected
Beowulf technology originally developed
The vast majority supercomputer cluster
Because grid computing system described easily handle embarrassingly parallel problem modern cluster typically designed handle difficult require node share intermediate result often
This requires high bandwidth importantly interconnection network
Many historic current supercomputer use customized network hardware specifically designed cluster computing Cray Gemini network
As current supercomputer use standard network hardware often
A massively parallel processor MPP single computer many networked processor
MPPs many characteristic cluster MPPs specialized interconnect network whereas cluster use commodity hardware networking
MPPs also tend larger cluster typically far processor
In MPP CPU contains memory copy operating system application
Each subsystem communicates others via interconnect
fifth fastest world according June ranking MPP
Grid computing distributed form parallel computing
It make use computer communicating work given problem
Because low bandwidth extremely high latency available Internet distributed computing typically deal problem
created example
Most grid computing application use software sits operating system application manage network resource standardize software interface
The common distributed computing middleware BOINC
Often distributed computing software make use spare cycle performing computation time computer idling
Within parallel computing specialized parallel device remain niche area interest
While tend applicable class parallel problem
use FPGA computer
An FPGA essence computer chip rewire given task
FPGAs programmed
However programming language tedious
Several vendor created language attempt emulate syntax semantics programmer familiar
The best known C HDL language
Specific subset based also used purpose
AMD decision open technology vendor become enabling technology reconfigurable computing
According Michael Chief Operating Officer first walked AMD called u stealer
Now call u partner
computing GPGPU fairly recent trend computer engineering research
GPUs heavily optimized processing
Computer graphic processing field dominated data parallel operation
In early day GPGPU program used normal graphic APIs executing program
However several new programming language platform built general purpose computation GPUs releasing programming environment respectively
Other GPU programming language include
Nvidia also released specific product computation
The technology consortium Khronos Group released specification framework writing program execute across platform consisting CPUs GPUs
others supporting
Several ASIC approach devised dealing parallel application
Because ASIC definition specific given application fully optimized application
As result given application ASIC tends outperform computer
However ASICs created
This process requires mask set extremely expensive
A mask set cost million US dollar
The smaller transistor required chip expensive mask
Meanwhile performance increase computing time described tend wipe gain one two chip generation
High initial cost tendency overtaken computing rendered ASICs unfeasible parallel computing application
However built
One example PFLOPS machine us custom ASICs simulation
A vector processor CPU computer system execute instruction large set data
Vector processor operation work linear array number vector
An example vector operation vector number
They closely related Flynn SIMD classification
computer became famous computer
However vector CPUs full computer generally disappeared
Modern include vector processing instruction SSE
created programming parallel computer
These generally divided class based assumption make underlying memory memory distributed memory shared distributed memory
Shared memory programming language communicate manipulating shared memory variable
Distributed memory us
two widely used shared memory APIs whereas MPI widely used system API
One concept used programming parallel program one part program promise deliver required datum another part program future time
also coordinating effort make HMPP directive open standard called
The OpenHMPP programming model offer syntax efficiently offload computation hardware accelerator optimize data movement hardware memory
OpenHMPP directive describe remote procedure call RPC accelerator device
GPU generally set core
The directive annotate code describe two set functionality offloading procedure denoted codelets onto remote device optimization data transfer CPU main memory accelerator memory
The rise consumer GPUs led support either graphic APIs referred dedicated APIs language extension
Automatic parallelization sequential program parallel computing
Despite decade work compiler researcher automatic parallelization limited success
Mainstream parallel programming language remain either best programmer give compiler parallelization
A fully implicit parallel programming language Parallel
As computer system grows complexity usually decrease
technique whereby computer system take snapshot record current resource allocation variable state akin information used restore program computer fail
Application checkpointing mean program restart last checkpoint rather beginning
While checkpointing provides benefit variety situation especially useful highly parallel system large number processor used
As parallel computer become larger faster becomes feasible solve problem previously took long run
Parallel computing used wide range field economics
Common type problem found parallel computing application Parallel computing also applied design particularly via system performing operation parallel
This provides case one component fail also allows automatic result differ
These method used help prevent single event upset caused transient error
Although additional measure may required embedded specialized system method provide cost effective approach achieve redundancy commercial system
The origin true MIMD parallelism go back
In April Gill Ferranti discussed parallel programming need branching waiting
Also IBM researcher discussed use parallelism numerical calculation first time
introduced computer accessed memory module
In Amdahl Slotnick published debate feasibility parallel processing American Federation Information Processing Societies Conference
It debate coined define limit due parallelism
In company introduced first Multics system symmetric multiprocessor system capable running eight processor parallel
project among first multiprocessor processor
The first multiprocessor snooping cache
SIMD parallel computer traced back
The motivation behind early SIMD computer amortize processor multiple instruction
In Slotnick proposed building massively parallel computer
His design funded earliest SIMD effort
The key design fairly high parallelism processor allowed machine work large datasets would later known
However ILLIAC IV called infamous supercomputer project one fourth completed took year cost almost four time original estimate
When finally ready run first real application outperformed existing commercial supercomputer
In early started developing came known theory view biological brain
In Minsky published claim mind formed many little agent mindless
The theory attempt explain call intelligence could product interaction part
Minsky say biggest source idea theory came work trying create machine us robotic arm video camera computer build child block
Similar model also view biological brain massively parallel computer
brain made constellation independent agent also described

Stack Exchange network consists Q A community including largest trusted online community developer learn share knowledge build career
What difference term execution
I never quite able grasp distinction
The tag defines concurrency manner running two process simultaneously I thought parallelism exactly thing
separate thread process potentially run separate processor
Also consider something like asynchronous dealing concurrency parallelism
Concurrency parallelism two related distinct concept
Concurrency mean essentially task A task B need happen independently A start running B start A finished
There various different way accomplishing concurrency
One parallelism multiple CPUs working different task time
But way
Another work like Task A work certain point CPU working stop switch task B work switch back task A
If time slice small enough may appear user thing run parallel even though actually processed serial multitasking CPU
The two concept related different
mean two calculation happen within time frame usually sort dependency
mean two calculation happen simultaneously
Put boldly concurrency describes problem two thing need happen together parallelism describes solution two processor core used execute two thing simultaneously
Parallelism one way implement concurrency one
Another popular solution interleaved processing
coroutines split task atomic step switch back forth two
By far best known example concurrency JavaScript work one thread asynchronous callback wait previous chunk code finished executing
This important know guarantee function write atomic callback interrupt return
But also mean busy loop wo work ca set timeout loop fire loop prevent timeout callback executing
Concurrent execution generalized form parallel execution
For example parallel program also called concurrent reverse true
For detail read research paper Parallel processing subset concurrent processing
Concurrent processing describes two task occurring asynchronously meaning order task executed predetermined
Two thread run concurrently processor core interleaving executable instruction
For example thread run thread run etc
Parallel processing type concurrent processing one set instruction executing simultaneously
This could multiple system working common problem distributed computing multiple core system
In opinion application programming perspective difference two concept two word confusing confusion sake
I think thread interleaving brought simulate multicore processing back day multicore possibility
Why word outdated mindset
Mason Wheeler Penguin given answer
One Core task switching multicore concurrent strictly multicore parallel
My opinion two term rolled one I make effort avoid saying concurrent
I guess OS programming level distinction important application programmer perspective matter much
I written mapReduce Spark MPI cuda openCL multithreaded I never stop think job running interleaved thread multiple core
For example I write multithreaded sometimes I sure many core I get though way make demand many core get described
In spark I map reduce operation idea jvm handling hardware level
On GPUs I every thread assigned simple processor I always sync thread wherever problem might arise
With MPI communication machine specified explicitly could interleave function running multiple machine single core combine result via appropriate single threaded function
And use MPI coordinate bunch single core machine one multithreading
What difference make
I say none
Call parallel done
tdammer statement come close rest besides point
He say Put boldly concurrency describes problem two thing need happen together parallelism describes solution two processor core used execute two thing simultaneously Let analyse word
Current mean happening actual relevant moment
Con mean counter aligning
Parallel mean direction without crossing without eachother way
So concurrency implies competing resource
Parallelism
Parallel process may using resource considered problem issue
With concurrency issue dealt
Obviously term used differently different culture
My understanding following Parallelism way speed processing
Whether matrix multiplication single core multiple core even GPU outcome else program broken
It add new functionality program speed
While concurrency thing could sequentially
For example serving different webpage time client waiting next request
Though could simulate degree interleaving done elder day
Note behaviour concurrent program nondeterministic
It example clear client completly served first
You could run quite test get different result time regarding order request finished
The system guarantee client served b reasonable amount time
Usually work horse parallel computation aware care parallelism
While concurrent task often explicitly employ communication blocking queue synchronization locking mechanism
Thank interest question
Because attracted spam answer removed posting answer requires site
Would like answer one instead
asked viewed active site design logo Stack Exchange Inc user contribution licensed

computer scientist Internet entrepreneur
He chief executive officer Sporcle
He best known early employee key technical contributor original Napster
Aydar bought Fanning first book programming language would use two year later build Napster software
January April American computer scientist
She founded Institute Women Technology Anita Borg Institute Women Technology
While Digital Equipment developed patented method generating complete address trace analyzing designing memory system
born August Canadian computer scientist best known work programming language compiler related algorithm textbook art science computer programming
Aho received
Engineering Physics University Toronto
born December Danish computer scientist notable creation development widely used programming language
He Distinguished Research Professor hold College Engineering Chair Computer Science
born October American business magnate philanthropist investor computer programmer inventor
Gates former chief executive chairman Microsoft world largest software company Paul Allen
born Minneapolis Minnesota American computer scientist
He graduated Purdue University BS EE started computing career wiring programming IBM hybrid mechanical electronic Card Programmed Allison Division General Motors
born American technologist creator JavaScript scripting language
He cofounded Mozilla project Mozilla Foundation Mozilla Corporation served Mozilla Corporation chief technical officer briefly chief executive officer
born American software engineer Distinguished Professor Computer Science Industrial Systems Engineering TRW Professor Software Engineering
He known many contribution area software engineering
born November The Hague Netherlands computer scientist
He studied mathematics University Groningen wrote PhD thesis Rapid user interface development script language Gist
In joined World Wide Web Consortium work Cascading Style Sheets CSS
born American Software Engineer worked Sun Microsystems later Oracle Corporation following acquisition Sun
Cantrill included list development DTrace function OS Solaris provides mean tracing diagnosis software
He currently Chief Technology Officer Joyent
FRS December October English polymath
He mathematician philosopher inventor mechanical engineer best remembered originating concept programmable computer
September October American computer scientist
He created C programming language colleague Ken Thompson Unix operating system
Ritchie Thompson received Turing Award ACM American computer scientist
He one small group helped develop system Stanford later resulted Sun Microsystems later founder Silicon Graphics
He define application binary interface Solaris Sun principal system software product
August April English computer scientist working IBM invented relational model database management theoretical basis relational database
He made valuable contribution computer science
born August American computer scientist pioneer field optimizing compiler
Her achievement include seminal work compiler code optimization parallelization
She also role intelligence work programming language
born August American electrical engineer manager
An early employee Digital Equipment Corporation DEC Bell designed several PDP machine later became Vice President Engineering overseeing development VAX
born
November Argentina mathematician computer scientist
Beginning late Chaitin made contribution algorithmic information theory metamathematics particular result equivalent Godel incompleteness theorem
OC born May Canadian computer scientist best known father Java programming language
In Gosling received Bachelor Science Computer Science University Calgary
December March American computer scientist
He directed team invented first widely used programming language FORTRAN inventor form BNF widely used notation define formal language syntax
born November Marconi Professor Communications Systems Computer Laboratory University Cambridge
Professor Jon Crowcroft distinguished many seminal contribution development Internet
His work satellite link interconnection technique paved way rural broadband
born March American business magnate computer scientist Google alongside Sergey Brin
On April Page succeeded Eric Schmidt chief executive officer Google
As Page personal wealth estimated US
billion ranking Forbes list billionaire
born September computer programmer author widely known creator Perl programming language Camelia spunky spokesbug Perl
Wall grew south Los Angeles Bremerton Washington starting higher education Seattle Pacific University
born December Finnish American software engineer well known architect development Linux kernel
He honored along Shinya Yamanaka Millennium Technology Prize Technology Academy Finland recognition creation new open source operating system computer leading widely used Linux kernel
born Guatemalan entrepreneur associate professor Computer Science Department Carnegie Mellon University
He known one pioneer crowdsourcing
He founder company reCAPTCHA sold Google CEO Duolingo popular platform
FRS Italian computer scientist Assistant Director Microsoft Research Cambridge UK
Cardelli well known research type theory operational semantics
Among contribution helped design implemented first compiler functional programming language ML
born February American business magnate investor philanthropist author
He known founder CEO Dell one world leading seller personal computer PCs
November August Professor Massachusetts Institute Technology Director
Laboratory Computer Science LCS
During Dertouzos term LCS innovated variety area including RSA encryption spreadsheet NuBus X Window System Internet
June November British computer scientist credited several important development computing
At time death Wilkes Emeritus Professor University Cambridge
He received number distinction
born Professor Artificial Intelligence University Bristol recipient Royal Society Wolfson Research Merit Award
His research contribution across different area machine learning artificial intelligence bioinformatics
Specifically work concentrated statistical analysis learning algorithm
June August known Don Estridge led development original IBM Personal Computer PC thus known father IBM PC
His decision dramatically changed computer industry resulting vast increase number personal computer sold bought
November Scharnhausen today part Ostfildern May Echterdingen today part German priest inventor
In devised precision sundial heliochronometer incorporated correction equation time
November July computer scientist known concurrent programming theory
In research computer science focused concurrent programming Inspired Dahl Kristen Nygaard programming language Simula invented monitor concept
born November Greenlandic programmer Canadian citizenship
He created PHP scripting language authoring first two version language participating development later version led group developer including Jim Winstead
born March American software freedom activist computer programmer
He best known launching GNU Project founding Free Software Foundation developing GNU Compiler Collection GNU Emacs writing GNU General Public License
retired professor computer science mathematics philosophy The University Texas Austin
He J Strother Moore invented string search algorithm particularly efficient string searching algorithm
He Moore also collaborated automated theorem prover Nqthm
born December Amercian engineer one father Internet sharing title American Internet pioneer Vint Cerf
In December Kahn Cerf received National Medal Technology founding developing Internet
Kahn protocol important communication protocol Internet
He responsible originating DARPA Internet program
In Kahn recipient prestigious ACM Alan Turing award
In received Presidential Medal Freedom highest civilian award United States
born Chandigarh December
He grew Bangalore early education Bishop School Pune St Joseph Boys High School Bangalore
Sabeer Bhatia Indian American entrepreneur founded Hotmail email service Jaxt
born August American computer scientist internet entrepreneur Larry Page Google one profitable Internet company
As June personal wealth estimated US billion
Together Brin Page percent company
computer scientist working area data management database theory finite model theory
He currently senior researcher Institute national de recherche en informatique et en automatique INRIA French national research institute focussing computer science related area professor College de France
Better known Apple Computers Steve Jobs American inventor pioneered microcomputer revolution
Born San Francisco California Jobs laid foundation Apple Computers along partner Steve Wozniak
Under Steve leadership Apple launched series revolutionary product iPhone iPod iPad major contribution modern technology
Steve also known architect Macintosh widely popular operating system catalyzed mass production computer GUI
Steve passed away long battle pancreatic cancer
born June also known TimBL British computer scientist best known inventor World Wide Web
Berners Lee director World Wide Web Consortium oversees Web continued development
born June American internet pioneer recognized one father Internet sharing title American engineer Bob Kahn
His contribution acknowledged lauded repeatedly
He instrumental development first commercial email system MCI Mail connected Internet
Copyright
All Rights Reserved

learn share knowledge build career
What difference concurrent programming parallel programing
I asked google find anything helped understand difference
Could give example
For I found explanation concurrency property program v parallel execution property machine enough still I ca say
If program using thread concurrent programming necessarily going executed parallel execution since depends whether machine handle several thread
Here visual example
Threads machine Threads threaded machine The dash represent executed code
As see split execute separately threaded machine execute several separate piece
Concurrent programming regard operation appear overlap primarily concerned complexity arises due control flow
The quantitative cost associated concurrent program typically throughput latency
Concurrent program often IO bound always
concurrent garbage collector entirely
The pedagogical example concurrent program web crawler
This program initiate request web page accepts response concurrently result downloads become available accumulating set page already visited
Control flow response necessarily received order time program run
This characteristic make hard debug concurrent program
Some application fundamentally concurrent
web server must handle client connection concurrently
Erlang perhaps promising upcoming language highly concurrent programming
Parallel programming concern operation overlapped specific goal improving throughput
The difficulty concurrent programming evaded making control flow deterministic
Typically program spawn set child task run parallel parent task continues every subtask finished
This make parallel program much easier debug
The hard part parallel programming performance optimization respect issue granularity communication
The latter still issue context multicores considerable cost associated transferring data one cache another
Dense multiply pedagogical example parallel programming solved efficiently using Straasen algorithm attacking parallel
Cilk perhaps promising language parallel programming computer including multicores
Concurrent Two queue one coffee machine
Parallel Two queue two coffee machine
I believe concurrent programming refers multithreaded programming letting program run multiple thread abstarcted hardware detail
Parallel programming refers specifically designing program algorithm take advantage available parallel execution
For example execute parallel two branch algorithm expectation hit result sooner average would first checked first second branch
Interpreting original question instead
In independently
The second computation wait first finished advance
It state however mechanism achieved
In setup suspending alternating thread required also called multithreading
In literally time
This possible single CPU requires setup instead
versus According
In view processor It described pic I found content blog
Thought useful relevant
Concurrency parallelism NOT thing
Two task concurrent order two task executed time predetermined may executed finished may executed finished may executed simultaneously instance time parallelism may executed alternatively
If two concurrent thread scheduled OS run one processor may get concurrency parallelism
Parallelism possible distributed system
Concurrency often referred property program concept general parallelism
Source In programming concurrency composition independently executing process parallelism simultaneous execution possibly related computation
Andrew Gerrand And Concurrency composition independently executing computation
Concurrency way structure software particularly way write clean code interacts well real world
It parallelism
Concurrency parallelism although enables parallelism
If one processor program still concurrent parallel
On hand concurrent program might run efficiently parallel multiprocessor
That property could important Rob Pike To understand difference I strongly recommend see Rob Pike one Golang creator video
parallel programming happens code executed time execution independent
Therefore usually preocupation shared variable wo likelly happen
However concurrent programming consists code executed diferent share variable therefore concurrent programming must establish sort rule decide wich executes first want sure consistency know certainty happen
If control thread compute time store thing variable would know expect end
Maybe thread faster maybe one thread even stopped middle execution another continued different computation corrupted yet fully computed variable possibility endless
It situation usually use concurrent programming instead parallel
Classic scheduling task Analysis show task known sequence tricked order
Analysis show task
Analysis show
We careless analysed matter therefore execute task using available facility time
Often scheduling available known event I called state change
People often think software Software system little slow uptake interested many principally innovative second none feature incl
explicit language support execution constructor language principally suffer forthcomming era Massive Parallel Processor Arrays available recent year wheel InMOS Transputers used year ago
Succinctly address following THE VERB What
THE NOUN What
Initiation schedule state change Once know thing happen say happen
Is way
Is way
Is best way
last least Good luck They two phrase describe thing slightly different viewpoint
Parallel programming describing situation viewpoint hardware least two processor possibly within single physical package working problem parallel
Concurrent programming describing thing viewpoint software two action may happen exactly time concurrently
The problem people trying use two phrase draw clear distinction none really exists
The reality dividing line trying draw fuzzy indistinct decade grown ever indistinct time
What trying discus fact upon time computer single CPU
When executed multiple process thread single CPU CPU really executing one instruction one thread time
The appearance concurrency illusion CPU switching executing instruction different thread quickly enough human perception anything le m look instantaneous looked like many thing
The obvious contrast computer multiple CPUs CPU multiple core machine executing instruction multiple thread process exactly time code executing one effect code executing
Now problem clean distinction never existed
Computer designer actually fairly intelligent noticed long time ago example needed read data device disk took time term CPU cycle finish
Instead leaving CPU idle happened figured various way letting one make request let code execute CPU request completed
So long CPUs became norm operation multiple thread happening parallel
That tip iceberg though
Decades ago computer started providing another level parallelism well
Again fairly intelligent people computer designer noticed lot case instruction affect possible execute one instruction stream time
One early example became pretty well known Control Data
This fairly wide margin fastest computer earth introduced much basic architecture remains use today
It tracked resource used instruction set execution unit executed instruction soon resource depended became available similar design recent processor
But commercial used say wait
There yet another design element add still confusion
It given quite different name Hyperthreading SMT CMP refer basic idea CPU execute multiple thread simultaneously using combination resource independent thread resource shared thread
In typical case combined parallelism outlined
To two set architectural register
Then set execution unit execute instruction soon necessary resource become available
These often combine well instruction separate stream virtually never depend resource
Then course get modern system multiple core
Here thing obvious right
We N somewhere moment separate core execute instruction time case real parallelism executing instruction one affect executing instruction another
Well sort
Even independent resource register execution unit least one level cache shared resource typically least lowest level cache definitely memory controller bandwidth memory
To summarize simple scenario people like contrast shared resource independent resource virtually never happen real life
With resource shared end something like run one program time stop running one run
With completely independent resource N computer running without even network connect ability share anything even share file well shared resource violation basic premise nothing shared
Every interesting case involves combination independent resource shared resource
Every reasonably modern computer lot modern least ability carry least independent operation simultaneously anything sophisticated taken advantage least degree
The nice clean division concurrent parallel people like draw exist almost never
What people like classify concurrent usually still involves least one often different type parallel execution
What like classify parallel often involves sharing resource example one process blocking another execution using resource shared two
People trying draw clean distinction parallel concurrent living fantasy computer never actually existed
I understood difference Concurrent running tandem using shared resource Parallel running side side using different resource So two thing happening time independent even come together point two thing drawing reserve throughout operation executed
Although complete agreement distinction term many author make following distinction So parallel program concurrent program multitasking operating system also concurrent even run machine one core since multiple task progress instant
An introduction parallel programming Concurrency property program parallel execution property machine
What concurrent part executed parallel answered exact hardware known
Which I might like add lead unhappy conclusion dealing explicit parallel programming There guarantee efficiency portability explicit parallel program
general sense refer environment task define occur order
One task occur another task performed time
specifically refer simultaneous execution concurrent task different processor
Thus parallel programming concurrent concurrent programming parallel
Source Thank interest question
Because attracted spam answer removed posting answer requires site
Would like answer one instead
asked viewed active site design logo Stack Exchange Inc user contribution licensed

 obj endobj obj stream hbbd r  i
endstream endobj startxref EOF obj stream hb AV   bJ     endstream endobj obj endobj obj endobj obj stream c  T j i   xR B       endstream endobj obj stream  J  Bt K mx QdN d VHT LO DX   G J E
endstream endobj obj stream j V g  R BjGA np L 
endstream endobj obj stream Y B    hu  V D endstream endobj obj stream U T jN MBq  GNd C endstream endobj obj stream
PgzY ro b endstream endobj obj stream W F    Z  endstream endobj obj stream
P
  v endstream endobj obj stream g
I Sq
J q  endstream endobj obj stream
Qf
K Qd

E  I B

Hui Shen Mark Robinson Graphical representation scenario using Combined Fragments UML Sequence Diagrams serve mean expressing aggregation multiple trace encompassing complex concurrent behavior
However Combined Fragments increase difficulty analysis scenario
This paper introduces approach formally verify Combined Fragments nested Combined Fragments using model checking
Shen Hui Robinson Mark
TY CHAP Model Checking Combined Fragments Sequence Diagrams AU Shen Hui AU Robinson Mark AU Niu Jianwei PY Graphical representation scenario using Combined Fragments UML Sequence Diagrams serve mean expressing aggregation multiple trace encompassing complex concurrent behavior
However Combined Fragments increase difficulty analysis scenario
This paper introduces approach formally verify Combined Fragments nested Combined Fragments using model checking
AB Graphical representation scenario using Combined Fragments UML Sequence Diagrams serve mean expressing aggregation multiple trace encompassing complex concurrent behavior
However Combined Fragments increase difficulty analysis scenario
This paper introduces approach formally verify Combined Fragments nested Combined Fragments using model checking
KW Concurrency Communication KW Model Checking KW Modeling KW Sequence Diagram UR http UR http DO Conference contribution SN VL CCIS Communications Computer Information Science SP EP BT Software Data Technologies International Conference ICSOFT Revised Selected Papers PB Springer Verlag ER Powered
Cookies used site
To decline learn visit

For question International pricing please go page
Principles Parallelism The Principles Concurrency course examine design implementation concurrency abstraction found modern programming language computing system
In course concurrency viewed primarily program structuring device
Refine search using tool

Flowcharts ideal diagram visually representing business process
For example need show flow process various department within organization use flowchart
This paper provides visual representation basic flowchart symbol proposed use communicating structure web site well correlation developing instructional project
A typical flowchart older Computer Science textbook may following kind symbol
may contain symbol usually represented circle represent converging path flow chart
Circles one arrow coming one going
Some flow chart may arrow pointing another arrow instead
These useful represent iterative process Computer Science called loop
A loop may example consists connector control first enters processing step conditional one arrow exiting loop one going back connector
connector often used signify connection part process held another sheet screen
A flowchart described page divided different lane describing control different organization unit
An unit appearing particular lane within control organizational unit
This technique allows analyst locate responsibility performing action making decision correctly allowing relationship different organizational unit responsibility single process
Flowcharts use special shape represent different type action step process
Lines arrow show sequence step relationship
See use instantly
refer individual web page may may contain multiple element
Workflow relationship work done different department fixed sequence
This mean one department need finish job work continue another department
The development maintenance work flow relationship important manager depend preceding area work responsible manager worker different stage chain
The following shape similar basic flowchart symbol specially used audit flowchart
Edraw Max perfect flowchart organizational chart mind map also network diagram floor plan workflow fashion design UML diagram electrical diagram science illustration chart graph beginning

 obj R R endobj obj R R endobj obj R R R endobj obj R R R endobj obj R R endobj obj R R R R R endobj obj R R endobj obj R R endobj obj R R R R R R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R R endobj obj R R endobj obj R R R R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R R R endobj obj R R R R R R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R R endobj obj R R R endobj obj R R endobj obj R R R R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R R R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R R endobj obj R R endobj obj R R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R R endobj obj R R R R R R R R R R R R endobj obj R R R R R R R endobj obj R R endobj obj R R endobj obj R R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R R endobj obj R R R endobj obj R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R R R R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R R R R R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R R R endobj obj R R endobj obj R R R R R R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R R endobj obj R R R endobj obj R R endobj obj R R R R R R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R R endobj obj R R R endobj obj R R endobj obj R R R R R R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R R R R R R R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R endobj obj R R R R endobj obj R R endobj obj R R endobj obj R R R R R R endobj obj R R endobj obj R R R R endobj obj endobj obj R R R endobj obj R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R endobj obj R R endobj obj R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R endobj obj R R endobj obj R R endobj obj R R R R R R R R R R R R R R R R R endobj obj R R endobj obj R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R R R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R endobj obj R R R R R endobj obj R R endobj obj R R endobj obj R R true false false false false false false R endobj obj R R R R endobj obj R R R R R R R R R R R R R R R R R R R R R R R R R R R endobj obj stream  hWmFv  AF b    o mV  rc o  z  F Wn

